# Foundations-of-Graphical-Models-STCS-GR6701-2020

Variational Autoencoders (VAEs) are among the most popular probabilistic generative models of recent times, achieving stellar success in a variety of unsupervised learning tasks. The hierarchical nature of modelling used by VAEs often leads to very complicated and intractable likelihood functions. Unable to maximize the likelihood directly, VAE training generally proceeds through maximization of a lower bound to the log-likelihood, known as the Evidence Lower Bound (ELBO). Very recently, \cite{sum} obtained a closed form expression for the ELBO at convergence to any stationary point, and in particular to any local maxima or minima of the ELBO objective. Their results show that the ELBO is expressible as a linear combination of the (negative) entropy of the latent/prior distribution, the expected (negative) entropy of the observable sampling distribution, and the average entropy of the variational distributions. Currently, their results are applicable to standard VAEs with Gaussian priors on continuous latent variables. 

## Our Contributions:

We extend the existing results for continuous Gaussian latents to the case of discrete binary latents used by "discrete" autoencoders. Specifically, we prove a similar result for the discrete autoencoder of DVAE (Rolfe 2017). Here, in addition to the binary latents, a set of continuous Gaussian latents is also used by augmenting them to the binary latents. This is done so as to make differentiation and consequently optimization of the variational autoencoder using backpropagation possible. Our derivations show that even in such complicated models, the convergence result expressing the ELBO objective function value as the sum of appropriate entropies of distributions still holds.Although this is primarily a result of theoretical interest, we perform rigorous experiments to illustrate the veracity of the result, and show the convergence of the ELBO to the linear combination of entropies. Finally, we attempt to explain the cause and nature of posterior collapse in VAEs through means of this newly obtained result, and propose a possible diagnostic measure to rule out posterior collapse. This illustrates the utility of this result in improving the current understanding of the theory and practice relating to Variational Autoencoders.
